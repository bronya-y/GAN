{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAN_MNIST2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOiOC9sZTq0WH6mnAuOQTmE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bronya-y/GAN/blob/main/GAN_MNIST2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooQgN1keVwhH"
      },
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "import pickle\n",
        "import imageio\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4mj_zgIbVzEn"
      },
      "source": [
        "\n",
        "# G(z)\n",
        "class generator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self, input_size=32, n_class = 10):\n",
        "        super(generator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 256)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, 512)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, 1024)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, n_class)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input):\n",
        "        x = F.leaky_relu(self.fc1(input), 0.2)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.tanh(self.fc4(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "class discriminator(nn.Module):\n",
        "    # initializers\n",
        "    def __init__(self, input_size=32, n_class=10):\n",
        "        super(discriminator, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, 1024)\n",
        "        self.fc2 = nn.Linear(self.fc1.out_features, 512)\n",
        "        self.fc3 = nn.Linear(self.fc2.out_features, 256)\n",
        "        self.fc4 = nn.Linear(self.fc3.out_features, n_class)\n",
        "\n",
        "    # forward method\n",
        "    def forward(self, input):\n",
        "        x = F.leaky_relu(self.fc1(input), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc2(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.leaky_relu(self.fc3(x), 0.2)\n",
        "        x = F.dropout(x, 0.3)\n",
        "        x = F.sigmoid(self.fc4(x))\n",
        "\n",
        "        return x"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJvwRqhjV2Kv",
        "outputId": "557c3f65-2c0d-40c3-aeb5-f6b34142d6df"
      },
      "source": [
        "fixed_z_ = torch.randn((5 * 5, 100))    # fixed noise\n",
        "fixed_z_ = Variable(fixed_z_.cuda(), volatile=True)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_lRNp6CV-Uo"
      },
      "source": [
        "\n",
        "def show_result(num_epoch, show = False, save = False, path = 'result.png', isFix=False):\n",
        "    z_ = torch.randn((5*5, 100))\n",
        "    z_ = Variable(z_.cuda(), volatile=True)\n",
        "\n",
        "    G.eval()\n",
        "    if isFix:\n",
        "        test_images = G(fixed_z_)\n",
        "    else:\n",
        "        test_images = G(z_)\n",
        "    G.train()\n",
        "\n",
        "    size_figure_grid = 5\n",
        "    fig, ax = plt.subplots(size_figure_grid, size_figure_grid, figsize=(5, 5))\n",
        "    for i, j in itertools.product(range(size_figure_grid), range(size_figure_grid)):\n",
        "        ax[i, j].get_xaxis().set_visible(False)\n",
        "        ax[i, j].get_yaxis().set_visible(False)\n",
        "\n",
        "    for k in range(5*5):\n",
        "        i = k // 5\n",
        "        j = k % 5\n",
        "        ax[i, j].cla()\n",
        "        ax[i, j].imshow(test_images[k, :].cpu().data.view(28, 28).numpy(), cmap='gray')\n",
        "\n",
        "    label = 'Epoch {0}'.format(num_epoch)\n",
        "    fig.text(0.5, 0.04, label, ha='center')\n",
        "    plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()\n",
        "\n",
        "def show_train_hist(hist, show = False, save = False, path = 'Train_hist.png'):\n",
        "    x = range(len(hist['D_losses']))\n",
        "\n",
        "    y1 = hist['D_losses']\n",
        "    y2 = hist['G_losses']\n",
        "\n",
        "    plt.plot(x, y1, label='D_loss')\n",
        "    plt.plot(x, y2, label='G_loss')\n",
        "\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.legend(loc=4)\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "\n",
        "    if save:\n",
        "        plt.savefig(path)\n",
        "\n",
        "    if show:\n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V80GXaEoWkq8"
      },
      "source": [
        "# training parameters\n",
        "batch_size = 128\n",
        "lr = 0.0002\n",
        "train_epoch = 100\n",
        "\n",
        "# data_loader\n",
        "transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.1307,), (0.3081,))\n",
        "        # transforms.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5))\n",
        "])\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.MNIST('data', train=True, download=True, transform=transform),\n",
        "    batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# network\n",
        "G = generator(input_size=100, n_class=28*28)\n",
        "D = discriminator(input_size=28*28, n_class=1)\n",
        "G.cuda()\n",
        "D.cuda()\n",
        "\n",
        "# Binary Cross Entropy loss\n",
        "BCE_loss = nn.BCELoss()\n",
        "\n",
        "# Adam optimizer\n",
        "G_optimizer = optim.Adam(G.parameters(), lr=lr)\n",
        "D_optimizer = optim.Adam(D.parameters(), lr=lr)\n",
        "\n",
        "# results save folder\n",
        "if not os.path.isdir('MNIST_GAN_results'):\n",
        "    os.mkdir('MNIST_GAN_results')\n",
        "if not os.path.isdir('MNIST_GAN_results/Random_results'):\n",
        "    os.mkdir('MNIST_GAN_results/Random_results')\n",
        "if not os.path.isdir('MNIST_GAN_results/Fixed_results'):\n",
        "    os.mkdir('MNIST_GAN_results/Fixed_results')\n",
        "\n",
        "train_hist = {}\n",
        "train_hist['D_losses'] = []\n",
        "train_hist['G_losses'] = []"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6QImPpuLVrkr",
        "outputId": "53e19383-1961-43a8-8515-80e403f12385"
      },
      "source": [
        "for epoch in range(train_epoch):\n",
        "    D_losses = []\n",
        "    G_losses = []\n",
        "    for x_, _ in train_loader:\n",
        "        # print(x_.shape)\n",
        "        # train discriminator D\n",
        "        D.zero_grad()\n",
        "\n",
        "        x_ = x_.view(-1, 28 * 28)\n",
        "\n",
        "        mini_batch = x_.size()[0]\n",
        "\n",
        "        y_real_ = torch.ones(mini_batch,1)\n",
        "        y_fake_ = torch.zeros(mini_batch,1)\n",
        "\n",
        "        x_, y_real_, y_fake_ = Variable(x_.cuda()), Variable(y_real_.cuda()), Variable(y_fake_.cuda())\n",
        "        D_result = D(x_)\n",
        "        D_real_loss = BCE_loss(D_result, y_real_)\n",
        "        D_real_score = D_result\n",
        "\n",
        "        z_ = torch.randn((mini_batch, 100))\n",
        "        z_ = Variable(z_.cuda())\n",
        "        G_result = G(z_)\n",
        "\n",
        "        D_result = D(G_result)\n",
        "        D_fake_loss = BCE_loss(D_result, y_fake_)\n",
        "        D_fake_score = D_result\n",
        "\n",
        "        D_train_loss = D_real_loss + D_fake_loss\n",
        "\n",
        "        D_train_loss.backward()\n",
        "        D_optimizer.step()\n",
        "\n",
        "        D_losses.append(D_train_loss.item())\n",
        "\n",
        "        # train generator G\n",
        "        G.zero_grad()\n",
        "\n",
        "        z_ = torch.randn((mini_batch, 100))\n",
        "        y_ = torch.ones(mini_batch,1)\n",
        "\n",
        "        z_, y_ = Variable(z_.cuda()), Variable(y_.cuda())\n",
        "        G_result = G(z_)\n",
        "        D_result = D(G_result)\n",
        "        G_train_loss = BCE_loss(D_result, y_)\n",
        "        G_train_loss.backward()\n",
        "        G_optimizer.step()\n",
        "\n",
        "        G_losses.append(G_train_loss.item())\n",
        "\n",
        "    print('[%d/%d]: loss_d: %.3f, loss_g: %.3f' % (\n",
        "        (epoch + 1), train_epoch, torch.mean(torch.FloatTensor(D_losses)), torch.mean(torch.FloatTensor(G_losses))))\n",
        "    p = 'MNIST_GAN_results/Random_results/MNIST_GAN_' + str(epoch + 1) + '.png'\n",
        "    fixed_p = 'MNIST_GAN_results/Fixed_results/MNIST_GAN_' + str(epoch + 1) + '.png'\n",
        "    show_result((epoch+1), save=True, path=p, isFix=False)\n",
        "    show_result((epoch+1), save=True, path=fixed_p, isFix=True)\n",
        "    train_hist['D_losses'].append(torch.mean(torch.FloatTensor(D_losses)))\n",
        "    train_hist['G_losses'].append(torch.mean(torch.FloatTensor(G_losses)))\n",
        "\n",
        "\n",
        "print(\"Training finish!... save training results\")\n",
        "torch.save(G.state_dict(), \"MNIST_GAN_results/generator_param.pkl\")\n",
        "torch.save(D.state_dict(), \"MNIST_GAN_results/discriminator_param.pkl\")\n",
        "with open('MNIST_GAN_results/train_hist.pkl', 'wb') as f:\n",
        "    pickle.dump(train_hist, f)\n",
        "\n",
        "show_train_hist(train_hist, save=True, path='MNIST_GAN_results/MNIST_GAN_train_hist.png')\n",
        "\n",
        "images = []\n",
        "for e in range(train_epoch):\n",
        "    img_name = 'MNIST_GAN_results/Fixed_results/MNIST_GAN_' + str(e + 1) + '.png'\n",
        "    images.append(imageio.imread(img_name))\n",
        "imageio.mimsave('MNIST_GAN_results/generation_animation.gif', images, fps=5)\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1805: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
            "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:1794: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
            "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1/100]: loss_d: 1.077, loss_g: 2.068\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:4: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  after removing the cwd from sys.path.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2/100]: loss_d: 0.456, loss_g: 3.393\n",
            "[3/100]: loss_d: 0.331, loss_g: 5.679\n",
            "[4/100]: loss_d: 0.207, loss_g: 5.752\n",
            "[5/100]: loss_d: 0.168, loss_g: 5.778\n",
            "[6/100]: loss_d: 0.137, loss_g: 6.531\n",
            "[7/100]: loss_d: 0.128, loss_g: 6.345\n",
            "[8/100]: loss_d: 0.102, loss_g: 6.963\n",
            "[9/100]: loss_d: 0.080, loss_g: 6.586\n",
            "[10/100]: loss_d: 0.075, loss_g: 6.730\n",
            "[11/100]: loss_d: 0.074, loss_g: 6.538\n",
            "[12/100]: loss_d: 0.067, loss_g: 6.742\n",
            "[13/100]: loss_d: 0.061, loss_g: 6.762\n",
            "[14/100]: loss_d: 0.053, loss_g: 6.950\n",
            "[15/100]: loss_d: 0.061, loss_g: 6.755\n",
            "[16/100]: loss_d: 0.053, loss_g: 6.714\n",
            "[17/100]: loss_d: 0.053, loss_g: 7.063\n",
            "[18/100]: loss_d: 0.044, loss_g: 7.249\n",
            "[19/100]: loss_d: 0.045, loss_g: 7.346\n",
            "[20/100]: loss_d: 0.039, loss_g: 7.245\n",
            "[21/100]: loss_d: 0.042, loss_g: 7.226\n",
            "[22/100]: loss_d: 0.036, loss_g: 7.451\n",
            "[23/100]: loss_d: 0.031, loss_g: 7.661\n",
            "[24/100]: loss_d: 0.029, loss_g: 7.893\n",
            "[25/100]: loss_d: 0.030, loss_g: 7.729\n",
            "[26/100]: loss_d: 0.026, loss_g: 7.975\n",
            "[27/100]: loss_d: 0.026, loss_g: 8.099\n",
            "[28/100]: loss_d: 0.024, loss_g: 8.105\n",
            "[29/100]: loss_d: 0.023, loss_g: 8.452\n",
            "[30/100]: loss_d: 0.021, loss_g: 8.463\n",
            "[31/100]: loss_d: 0.020, loss_g: 8.569\n",
            "[32/100]: loss_d: 0.019, loss_g: 8.629\n",
            "[33/100]: loss_d: 0.018, loss_g: 8.846\n",
            "[34/100]: loss_d: 0.016, loss_g: 8.992\n",
            "[35/100]: loss_d: 0.016, loss_g: 9.025\n",
            "[36/100]: loss_d: 0.015, loss_g: 9.109\n",
            "[37/100]: loss_d: 0.015, loss_g: 9.353\n",
            "[38/100]: loss_d: 0.015, loss_g: 9.341\n",
            "[39/100]: loss_d: 0.013, loss_g: 9.114\n",
            "[40/100]: loss_d: 0.013, loss_g: 9.391\n",
            "[41/100]: loss_d: 0.012, loss_g: 9.597\n",
            "[42/100]: loss_d: 0.013, loss_g: 9.393\n",
            "[43/100]: loss_d: 0.013, loss_g: 9.695\n",
            "[44/100]: loss_d: 0.011, loss_g: 9.548\n",
            "[45/100]: loss_d: 0.012, loss_g: 9.384\n",
            "[46/100]: loss_d: 0.011, loss_g: 9.330\n",
            "[47/100]: loss_d: 0.008, loss_g: 9.923\n",
            "[48/100]: loss_d: 0.009, loss_g: 10.130\n",
            "[49/100]: loss_d: 0.009, loss_g: 10.181\n",
            "[50/100]: loss_d: 0.010, loss_g: 10.150\n",
            "[51/100]: loss_d: 0.009, loss_g: 10.273\n",
            "[52/100]: loss_d: 0.008, loss_g: 10.053\n",
            "[53/100]: loss_d: 0.008, loss_g: 10.349\n",
            "[54/100]: loss_d: 0.009, loss_g: 10.185\n",
            "[55/100]: loss_d: 0.009, loss_g: 10.695\n",
            "[56/100]: loss_d: 0.008, loss_g: 10.129\n",
            "[57/100]: loss_d: 0.008, loss_g: 10.432\n",
            "[58/100]: loss_d: 0.009, loss_g: 10.376\n",
            "[59/100]: loss_d: 0.008, loss_g: 10.606\n",
            "[60/100]: loss_d: 0.008, loss_g: 10.370\n",
            "[61/100]: loss_d: 0.008, loss_g: 10.375\n",
            "[62/100]: loss_d: 0.007, loss_g: 10.770\n",
            "[63/100]: loss_d: 0.007, loss_g: 10.408\n",
            "[64/100]: loss_d: 0.007, loss_g: 10.388\n",
            "[65/100]: loss_d: 0.007, loss_g: 10.770\n",
            "[66/100]: loss_d: 0.008, loss_g: 10.741\n",
            "[67/100]: loss_d: 0.007, loss_g: 10.710\n",
            "[68/100]: loss_d: 0.007, loss_g: 10.541\n",
            "[69/100]: loss_d: 0.006, loss_g: 10.874\n",
            "[70/100]: loss_d: 0.007, loss_g: 10.707\n",
            "[71/100]: loss_d: 0.006, loss_g: 10.963\n",
            "[72/100]: loss_d: 0.006, loss_g: 11.034\n",
            "[73/100]: loss_d: 0.007, loss_g: 10.866\n",
            "[74/100]: loss_d: 0.007, loss_g: 11.243\n",
            "[75/100]: loss_d: 0.005, loss_g: 11.089\n",
            "[76/100]: loss_d: 0.006, loss_g: 11.945\n",
            "[77/100]: loss_d: 0.006, loss_g: 11.167\n",
            "[78/100]: loss_d: 0.007, loss_g: 10.621\n",
            "[79/100]: loss_d: 0.006, loss_g: 11.046\n",
            "[80/100]: loss_d: 0.006, loss_g: 10.947\n",
            "[81/100]: loss_d: 0.007, loss_g: 11.508\n",
            "[82/100]: loss_d: 0.005, loss_g: 10.826\n",
            "[83/100]: loss_d: 0.005, loss_g: 10.584\n",
            "[84/100]: loss_d: 0.004, loss_g: 11.727\n",
            "[85/100]: loss_d: 0.007, loss_g: 11.777\n",
            "[86/100]: loss_d: 0.004, loss_g: 10.909\n",
            "[87/100]: loss_d: 0.004, loss_g: 11.685\n",
            "[88/100]: loss_d: 0.006, loss_g: 11.783\n",
            "[89/100]: loss_d: 0.006, loss_g: 10.796\n",
            "[90/100]: loss_d: 0.005, loss_g: 11.443\n",
            "[91/100]: loss_d: 0.003, loss_g: 11.766\n",
            "[92/100]: loss_d: 0.006, loss_g: 11.817\n",
            "[93/100]: loss_d: 0.006, loss_g: 11.332\n",
            "[94/100]: loss_d: 0.004, loss_g: 10.980\n",
            "[95/100]: loss_d: 0.004, loss_g: 11.126\n",
            "[96/100]: loss_d: 0.005, loss_g: 11.881\n",
            "[97/100]: loss_d: 0.005, loss_g: 11.166\n",
            "[98/100]: loss_d: 0.005, loss_g: 11.802\n",
            "[99/100]: loss_d: 0.004, loss_g: 11.970\n",
            "[100/100]: loss_d: 0.005, loss_g: 11.398\n",
            "Training finish!... save training results\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kEPeWrcaVvAM"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}